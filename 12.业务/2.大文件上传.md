### 怎么实现大文件上传，断点续传？


* handleUpload方法中，调用createFileChunk，返回文件切片fileChunkList。
* 用map遍历fileChunkList，并且给每个值加上hash，并且返回这个数组data
* 调用uploadChunks将每个切片放进formdata。再把每个formdata调用Promise.all并行发送
  * createFileChunk根据file.slice方法将文件切片，然后push到fileChunkList
* 完成上述过程后发送合并请求





### 大体思路

利用file的slice将文件分片，然后把每一个分片内容都放进formdata。为每一个formdata都创建一个XML请求。最后通过promise.all进行并发上传。



### 分片上传

* 如何拿到file对象

  * 通过监听input ，change事件。拿到对象事件，通过e.target.files拿到

* 如何分片
  * 如果小于10MB不分片，直接传输
  * 固定每一片大小为10MB。通过slice将文件分片，并且放入数组中
  * 需要封装一个XML请求，并且返回Promise
  * 遍历这个分片数组，将每个片放入`formdata`中，并且加上文件**hash_index**作文分片的名，并且文件名加index也发送。index将作为后端合并的顺序。再将`formdata`放入请求队列，通过Promise.all并发切片。后端拿到后通过index进行排序，然后合并切片

* await Promise.all()。发送完毕就发送一个合并请求

* 如何上传
  * 遍历分片列表(fileChunksList)，利用`formData.append()`
  * 将每一个`formdata`，遍历formdata，把每一个formdata都作为XML请求，然后通过**Promise.all并发发送**

* 怎么保证文件的唯一性
  * 使用`spark-md5`计算出文件的hash

  * 注意并不是每一个切片都计算一个hash，而是计算整个文件的hash

    * 为什么?

  * 拿到文件的分片数组，new一个filerReader对象，然后调用其`readAsBuffer`利用`fileReader`读取每个切片的ArrayBuffer，不断通过spark.apend传入sparkmd5中。当读取完后调用spark.end()进行hash生成。

    > spark-md5 需要根据所有切片才能算出一个 hash 值，不能直接将整个文件放入计算，否则即使不同文件也会有相同的 hash，具体可以看官方文档

  * 具体spark-md5怎么使用

    * 引入spark-md5.min.js

    * new一个spark实例。用到SparkMD5.ArrayBuffer()   spark = SparkMD5.ArrayBuffer()

    * 调用spark.apend()不断传入分片

    * 调用spark.end()生成hash

    * 为什么对分片不断进行spark.apend()。而不是直接计算整个文件hash。

      > **增量md5**对于散列大量数据（例如文件）的**性能**要好得多。可以使用FileReader＆Blob读取大块文件，并在保持**低内存使用率。**
      >
      > Incremental md5 performs a lot better for hashing large amounts of data, such as files. One could read files in chunks, using the FileReader & Blob's, and append each chunk for md5 hashing while keeping memory usage low. See example below.

* 分片如何合并

  * 后端拿到分片后通过文件名的**index进行排序后合并**，直接读取发送的数据可能顺序出错



### 进度显示

* 监听原生**xhr.upload.onprogress**

### 断点

* 怎么实现断点
  * 使用XML原生的`abort` 方法
  * 具体：由于是并发的，所以不能直接调用abort方法。需要先将我们前面生成的请求队列，然后在我们封装的xhr请求中将已经上传成功的xhr删除掉。具体通过在**xhr.onload** 事件最终对请求列表(requestList)调用findIndex 找出已经上传的xhr。`requestList.findIndex(item => item === xhr);`然后调用splice删除这个切片。当onload事件完成之后还需要将该xhr push回requestList。因为后面要根据他 + 以及上传的分片来判断切片是否全部上传
  * 点击暂停时对每一个现在requestList中的xhr都执行abort()方法。
* 怎么计算hash

### 续传

* 每次上传前调用一个接口，传入文件名和hash值。后端返回已经上传的文件`hash_index`的一个数组。前端通过`filter`方法从切片数组中去除已经上传的切片。
* 最后通过计算已上传切片 + 本次上传的切片 = 所有数组切片时，发送合并请求

### 秒传

* 上传前先根据文件的hash和文件名判断是否存在文件，如果存在就直接显示上传成功